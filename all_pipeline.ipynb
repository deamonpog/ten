{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11514d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from infomap import Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d83a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT_DIR = 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\TEST'\n",
    "INPUT_DIR = 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All'\n",
    "OUTPUT_DIR = 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ffa764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(data_directory):\n",
    "    \"\"\"\n",
    "    Reads data from all the csv files in the given directory\n",
    "    :param data_directory: Path to the directory that contains the csv files\n",
    "    :type data_directory: str\n",
    "    :return: pandas Dataframe that contains all the data from all csv files\n",
    "    :rtype: pd.Dataframe\n",
    "    \"\"\"\n",
    "    data_files = glob.glob(os.path.join(data_directory, \"*.csv*\"))\n",
    "    print(data_files)\n",
    "    df_list = []\n",
    "    for idx, file in enumerate(data_files):\n",
    "        print(f\"Reading {idx + 1} of {len(data_files)} files.\\nFile name: {file}\")\n",
    "        df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n",
    "                         dtype={'Twitter Author ID': str, 'Author':str,\n",
    "                                'Full Text':str, 'Title':str,\n",
    "                                'Thread Id':str, 'Thread Author':str,\n",
    "                                'Domain':str, 'Expanded URLs':str})\n",
    "        # df = df[['Date', 'Hashtags', 'Twitter Author ID', 'Author', 'Url', 'Thread Id', 'Thread Author', 'Domain']]\n",
    "        df = df.rename(columns={'Date':'datetime', 'Author': 'source_user_id',\n",
    "                                'Full Text':'content', 'Title':'title',\n",
    "                                'Thread Id': 'parent_source_msg_id', 'Thread Author': 'parent_source_user_id',\n",
    "                                'Domain':'platform', 'Expanded URLs':'article_url'})\n",
    "        df_list.append(df)\n",
    "    result_df = pd.concat(df_list).drop_duplicates()\n",
    "    return result_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13aab0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_01_to_2018_03_06_withFb_2033735572_MainQuery.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_07_to_2018_03_07_withoutFb_2033735852_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_07_to_2020_05_02_onlyFb_2033753991_MainQuery_FbIgOnly.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_08_to_2018_03_09_withoutFb_2033750044_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_10_to_2018_03_12_withFb_2033755725_MainQuery.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_13_to_2018_03_13_withoutFb_2033770110_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_14_to_2018_03_14_withoutFb_2033776708_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_15_to_2018_03_15_wihtoutFb_2033770779_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_16_to_2018_03_16_withoutFb_2033798850_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_17_to_2018_03_19_2033804694_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_20_to_2018_03_25_2033831837_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_26_to_2018_03_29_without_Fb_2033855430_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_03_30_to_2018_04_01_withoutFb_2033890094_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_02_to_2018_04_03_withoutFb_2033900386_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_04_to_2018_04_05_withoutFb_2033923235_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_06_to_2018_04_06_withoutFb_2033932671_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_07_to_2018_04_08_wihtout_Fb_2033954434_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_09_to_2018_04_11_withoutFb_2033968986_MIPs+test.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_12_to_2018_04_15_withoutFb_2033988828_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_16_to_2018_04_20_withoutFb_2034001178_MainQuery_withoutFb.csv.zip', 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\2018_04_21_to_2018_05_01_withoutFb_2034010661_MIPs+test.csv.zip']\n",
      "Reading 1 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_01_to_2018_03_06_withFb_2033735572_MainQuery.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (18,95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 2 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_07_to_2018_03_07_withoutFb_2033735852_MIPs+test.csv.zip\n",
      "Reading 3 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_07_to_2020_05_02_onlyFb_2033753991_MainQuery_FbIgOnly.csv.zip\n",
      "Reading 4 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_08_to_2018_03_09_withoutFb_2033750044_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 5 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_10_to_2018_03_12_withFb_2033755725_MainQuery.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (18,27,95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 6 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_13_to_2018_03_13_withoutFb_2033770110_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 7 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_14_to_2018_03_14_withoutFb_2033776708_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 8 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_15_to_2018_03_15_wihtoutFb_2033770779_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 9 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_16_to_2018_03_16_withoutFb_2033798850_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 10 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_17_to_2018_03_19_2033804694_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 11 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_20_to_2018_03_25_2033831837_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 12 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_26_to_2018_03_29_without_Fb_2033855430_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 13 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_03_30_to_2018_04_01_withoutFb_2033890094_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 14 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_02_to_2018_04_03_withoutFb_2033900386_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 15 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_04_to_2018_04_05_withoutFb_2033923235_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 16 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_06_to_2018_04_06_withoutFb_2033932671_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 17 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_07_to_2018_04_08_wihtout_Fb_2033954434_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (62,77,95,108,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 18 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_09_to_2018_04_11_withoutFb_2033968986_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 19 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_12_to_2018_04_15_withoutFb_2033988828_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_16_to_2018_04_20_withoutFb_2034001178_MainQuery_withoutFb.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 21 of 21 files.\n",
      "File name: C:\\STUFF\\RESEARCH\\Brandwatch\\DATA\\MainQuery\\All\\2018_04_21_to_2018_05_01_withoutFb_2034010661_MIPs+test.csv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ch132969\\AppData\\Local\\Temp/ipykernel_12700/2862529437.py:14: DtypeWarning: Columns (95,108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_files[idx], skiprows=6, parse_dates=['Date'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = read_csv_data(INPUT_DIR)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31022bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249377\n",
      "(249377, 3)\n",
      "                   platform  \\\n",
      "0               twitter.com   \n",
      "1               twitter.com   \n",
      "2               twitter.com   \n",
      "3               twitter.com   \n",
      "4               twitter.com   \n",
      "...                     ...   \n",
      "249372          twitter.com   \n",
      "249373         stallman.org   \n",
      "249374         stallman.org   \n",
      "249375  revolutionradio.org   \n",
      "249376     hotcopper.com.au   \n",
      "\n",
      "                                           source_user_id  user_id  \n",
      "0                                               melkaylan        0  \n",
      "1                                             MarkUrban01        1  \n",
      "2                                             prutter_pat        2  \n",
      "3                                              jimsciutto        3  \n",
      "4                                             optouttwice        4  \n",
      "...                                                   ...      ...  \n",
      "249372                                             arenda   249372  \n",
      "249373                                   richard stallman   249373  \n",
      "249374                                                NaN   249374  \n",
      "249375  Russia Exposes British Lies On Skripal, But Tr...   249375  \n",
      "249376                                             kbear1   249376  \n",
      "\n",
      "[249377 rows x 3 columns]\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# add user_id column and parent_user_id column\n",
    "\n",
    "def generate_users_dict(osn_msgs_df):\n",
    "    global next_user_id\n",
    "    users_data = {}\n",
    "    next_user_id = 0\n",
    "    def extract_user(row):\n",
    "        global next_user_id\n",
    "        if (row['platform'], row['source_user_id']) not in users_data:\n",
    "            users_data[(row['platform'], row['source_user_id'])] = next_user_id\n",
    "            next_user_id += 1\n",
    "        if (row['platform'], row['parent_source_user_id']) not in users_data:\n",
    "            users_data[(row['platform'], row['parent_source_user_id'])] = next_user_id\n",
    "            next_user_id += 1\n",
    "    df.apply(lambda row: extract_user(row), axis=1)\n",
    "    print(len(users_data))\n",
    "    return users_data\n",
    "\n",
    "users_data = generate_users_dict(df)\n",
    "\n",
    "users_df = pd.Series(users_data).rename_axis(['platform','source_user_id']).rename('user_id').reset_index()\n",
    "print(users_df.shape)\n",
    "print(users_df)\n",
    "users_df.to_csv(OUTPUT_DIR + \"\\\\users.csv\",index=False)\n",
    "\n",
    "# add user_id column and parent_user_id column\n",
    "df[['user_id','parent_user_id']] = df.apply(lambda row: pd.Series([\n",
    "            users_data[(row['platform'],row['source_user_id'])],\n",
    "            users_data[(row['platform'],row['parent_source_user_id'])]\n",
    "        ]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b3f503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Add news_domains column\n",
    "\n",
    "news_domains_csv_file = 'C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\news_outlets.xlsx - Sheet1.csv'\n",
    "skip_strings = {'add', 'al', 'au', 'ca', 'com', 'es', 'in', 'is', 'it', 'ms', 'my', 'net', 'news', 'org', 'rs', 'st', 'tv', 'uk', 'us', 'co'}\n",
    "\n",
    "def find_patterns_for_domains(in_news_domains_csv_file, in_skip_strings):\n",
    "    news_domains = pd.read_csv(in_news_domains_csv_file)['news outlets'].rename('news_outlets')\n",
    "    news_domains_set = set(news_domains.to_list())\n",
    "    pattern_to_news_domain_name = {re.compile(nd):nd for nd in news_domains_set}\n",
    "    for nd in news_domains_set:\n",
    "        valid_split_strs = set(nd.split('.'))\n",
    "        for e in in_skip_strings:\n",
    "            valid_split_strs.discard(e)\n",
    "        for sp in valid_split_strs:\n",
    "            if len(sp) > 2:\n",
    "                pattern_to_news_domain_name[re.compile(sp)] = nd\n",
    "    return pattern_to_news_domain_name\n",
    "\n",
    "def search_domain_in_string(in_expanded_url, in_news_domains_names):\n",
    "    # print(in_expanded_url)\n",
    "    max_len_match = None\n",
    "    max_len_found = 0\n",
    "    for ndn in in_news_domains_names:\n",
    "        match_obj = ndn.search(in_expanded_url)\n",
    "        if match_obj:\n",
    "            # print(match_obj, in_news_domains_names[ndn], match_obj.end() - match_obj.start())\n",
    "            if max_len_found < match_obj.end() - match_obj.start():\n",
    "                max_len_match = ndn\n",
    "                max_len_found = match_obj.end() - match_obj.start()\n",
    "    return in_news_domains_names[max_len_match] if max_len_match is not None else None\n",
    "\n",
    "\n",
    "def calculate_news_domain_series(in_string_series, in_skip_strings):\n",
    "    news_domains_names = find_patterns_for_domains(news_domains_csv_file, in_skip_strings)\n",
    "    return in_string_series.apply(lambda x: search_domain_in_string(x, news_domains_names) if type(x) is str else None)\n",
    "\n",
    "df['news_domain'] = calculate_news_domain_series(df['article_url'], skip_strings)\n",
    "# df = df[df['news_domain'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674b6314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        parent_user_id  user_id  num_retweets\n",
      "0                    1        0             1\n",
      "459259           42172    51357             1\n",
      "459260           42172    56840             1\n",
      "459261           42172   218390             1\n",
      "459263           42189    57986             1\n",
      "...                ...      ...           ...\n",
      "25069               30     1523           641\n",
      "86391               80     4385           706\n",
      "25945               30     4574           761\n",
      "261160            4068     4068           898\n",
      "139557             234      234          1672\n",
      "\n",
      "[706570 rows x 3 columns]\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate retweet network\n",
    "retweet_network_edges = df.groupby(['parent_user_id','user_id'], as_index=False).size().sort_values('size').rename(columns={'size':'num_retweets'})\n",
    "print(retweet_network_edges)\n",
    "retweet_network_edges.to_csv(OUTPUT_DIR + \"\\\\retweet_network.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a27ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 249377 nodes and 704901 edges\n",
      "7244\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate community partition on retweet network\n",
    "G = nx.from_pandas_edgelist(retweet_network_edges,'parent_user_id','user_id',['num_retweets'])\n",
    "print(G)\n",
    "lc = nx.algorithms.community.louvain_communities(G, seed=123)\n",
    "print(len(lc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08752965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6126\n",
      "plat_actors:\n",
      "       actor_id                   platform\n",
      "0            0                twitter.com\n",
      "1            1                youtube.com\n",
      "2            2         businessinsider.in\n",
      "3            3                 tumblr.com\n",
      "4            4  toinformistoinfluence.com\n",
      "...        ...                        ...\n",
      "6121      6121           imediaethics.org\n",
      "6122      6122              thestar.co.uk\n",
      "6123      6123              openews24.com\n",
      "6124      6124                 cpa.org.au\n",
      "6125      6125               stallman.org\n",
      "\n",
      "[6126 rows x 2 columns] \n",
      "\n",
      "6226\n",
      "indv_actors:\n",
      "     actor_id  user_id  received_share_count\n",
      "0       6126       30                137110\n",
      "1       6127      201                 28465\n",
      "2       6128      393                 21238\n",
      "3       6129    39607                 18329\n",
      "4       6130       11                 15949\n",
      "..       ...      ...                   ...\n",
      "95      6221    33634                  1512\n",
      "96      6222    28039                  1486\n",
      "97      6223    20088                  1483\n",
      "98      6224     2097                  1471\n",
      "99      6225    42815                  1466\n",
      "\n",
      "[100 rows x 3 columns] \n",
      "\n",
      "comm_actors:\n",
      "         user_id  comm_id  actor_id\n",
      "0         69280        0      6226\n",
      "1         57986        0      6226\n",
      "2         72135        0      6226\n",
      "3        173383        0      6226\n",
      "4        237128        0      6226\n",
      "...         ...      ...       ...\n",
      "249372    16179     7239     13465\n",
      "249373     3146     7240     13466\n",
      "249374     2226     7241     13467\n",
      "249375     7769     7242     13468\n",
      "249376     1730     7243     13469\n",
      "\n",
      "[249377 rows x 3 columns] \n",
      "\n",
      "Wall time: 600 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>plat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>plat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>plat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>plat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>plat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>13465</td>\n",
       "      <td>comm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>13466</td>\n",
       "      <td>comm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9946</th>\n",
       "      <td>13467</td>\n",
       "      <td>comm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11924</th>\n",
       "      <td>13468</td>\n",
       "      <td>comm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13407</th>\n",
       "      <td>13469</td>\n",
       "      <td>comm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actor_id actor_type\n",
       "0             0       plat\n",
       "1             1       plat\n",
       "2             2       plat\n",
       "3             3       plat\n",
       "4             4       plat\n",
       "...         ...        ...\n",
       "10495     13465       comm\n",
       "12475     13466       comm\n",
       "9946      13467       comm\n",
       "11924     13468       comm\n",
       "13407     13469       comm\n",
       "\n",
       "[13470 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# generate actor tables \n",
    "\n",
    "# generate platform actors table\n",
    "platform_actors_df = pd.DataFrame([[idx,plat] for idx,plat in enumerate(df['platform'].unique())], columns=['actor_id','platform'])\n",
    "next_actor_id = platform_actors_df['actor_id'].max() + 1\n",
    "print(next_actor_id)\n",
    "print(\"plat_actors:\\n\", platform_actors_df, \"\\n\")\n",
    "platform_actors_df.to_csv(OUTPUT_DIR + \"\\\\plat_actors.csv\", index=False)\n",
    "\n",
    "# generate individual actors table\n",
    "ind_actors_df = df['parent_user_id'].value_counts().iloc[:100].rename('received_share_count').rename_axis('user_id').reset_index().rename_axis('actor_id')\n",
    "ind_actors_df.index = ind_actors_df.index + next_actor_id\n",
    "ind_actors_df.reset_index(inplace=True)\n",
    "next_actor_id = ind_actors_df['actor_id'].max() + 1\n",
    "print(next_actor_id)\n",
    "print(\"indv_actors:\\n\", ind_actors_df, \"\\n\")\n",
    "ind_actors_df.to_csv(OUTPUT_DIR + \"\\\\indv_actors.csv\", index=False)\n",
    "\n",
    "# generate community actors table\n",
    "uid_to_comm = {}\n",
    "next_comm_id = 0\n",
    "for comm in lc:\n",
    "    for uid in comm:\n",
    "        uid_to_comm[uid] = next_comm_id\n",
    "    next_comm_id += 1\n",
    "        \n",
    "comm_actors_df = pd.Series(uid_to_comm).rename('comm_id').rename_axis('user_id').reset_index()\n",
    "comm_actors_df['actor_id'] = comm_actors_df['comm_id'] + next_actor_id\n",
    "print(\"comm_actors:\\n\", comm_actors_df, \"\\n\")\n",
    "comm_actors_df.to_csv(OUTPUT_DIR + \"\\\\comm_actors.csv\", index=False)\n",
    "\n",
    "# generate actors table\n",
    "all_actors = [[plat, 'plat'] for plat in platform_actors_df['actor_id']]\n",
    "all_actors = all_actors + [[indv, 'indv'] for indv in ind_actors_df['actor_id']]\n",
    "all_actors = all_actors + list({(comm, 'comm') for comm in comm_actors_df['actor_id']})\n",
    "all_actors_df = pd.DataFrame(all_actors, columns=['actor_id','actor_type']).sort_values('actor_id')\n",
    "all_actors_df.to_csv(OUTPUT_DIR + \"\\\\actors.csv\", index=False)\n",
    "all_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5ff2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query Id</th>\n",
       "      <th>Query Name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>Url</th>\n",
       "      <th>platform</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Page Type</th>\n",
       "      <th>Language</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>...</th>\n",
       "      <th>Region</th>\n",
       "      <th>Region Code</th>\n",
       "      <th>Root Blog Name</th>\n",
       "      <th>Root Post Id</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Subreddit Subscribers</th>\n",
       "      <th>Weblog Title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_user_id</th>\n",
       "      <th>news_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001100576</td>\n",
       "      <td>MainQuery</td>\n",
       "      <td>2018-03-06 23:59:40</td>\n",
       "      <td>RT @W7VOA Sergei Skripal, who is 66, was grant...</td>\n",
       "      <td>http://twitter.com/lindhays/statuses/971173499...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>Montana</td>\n",
       "      <td>USA.MT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lindhays (Democratic Socialism)</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001100576</td>\n",
       "      <td>MainQuery</td>\n",
       "      <td>2018-03-06 23:59:28</td>\n",
       "      <td>RT @Billbrowder Very disturbing additional fac...</td>\n",
       "      <td>http://twitter.com/Tesscatbird/statuses/971173...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>California</td>\n",
       "      <td>USA.CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tesscatbird (Tess)</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>theguardian.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001100576</td>\n",
       "      <td>MainQuery</td>\n",
       "      <td>2018-03-06 23:59:01</td>\n",
       "      <td>RT @Billbrowder Very disturbing additional fac...</td>\n",
       "      <td>http://twitter.com/crlulukat/statuses/97117333...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crlulukat (Kathy 🌼)</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>theguardian.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001100576</td>\n",
       "      <td>MainQuery</td>\n",
       "      <td>2018-03-06 23:58:36</td>\n",
       "      <td>RT @NBCNews \"The man went stiff. His arms stop...</td>\n",
       "      <td>http://twitter.com/tiffanyclay/statuses/971173...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tiffanyclay (❄️Evidence-Based Tiffany❄️)</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>nbcnews.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2001100576</td>\n",
       "      <td>MainQuery</td>\n",
       "      <td>2018-03-06 23:58:22</td>\n",
       "      <td>RT @Billbrowder Very disturbing additional fac...</td>\n",
       "      <td>http://twitter.com/KarCranky/statuses/97117317...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KarCranky (KarLen)</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>theguardian.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42590</th>\n",
       "      <td>2001099695</td>\n",
       "      <td>MIPs test</td>\n",
       "      <td>2018-04-21 00:08:03</td>\n",
       "      <td>Suspects in Attack on Ex-Spy Are in Russia, U....</td>\n",
       "      <td>http://twitter.com/ElizabethFieshe/statuses/98...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ElizabethFieshe (Pine Island Info Inc)</td>\n",
       "      <td>35405</td>\n",
       "      <td>30</td>\n",
       "      <td>nytimes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42596</th>\n",
       "      <td>2001099695</td>\n",
       "      <td>MIPs test</td>\n",
       "      <td>2018-04-21 00:03:22</td>\n",
       "      <td>Independent Swiss Lab Says BZ Toxin Used in Sk...</td>\n",
       "      <td>http://twitter.com/npnikk/statuses/98748188648...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>npnikk (ツ Nick)</td>\n",
       "      <td>24406</td>\n",
       "      <td>30</td>\n",
       "      <td>independent.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42597</th>\n",
       "      <td>2001099695</td>\n",
       "      <td>MIPs test</td>\n",
       "      <td>2018-04-21 00:02:57</td>\n",
       "      <td>Suspects in Attack on Ex-Spy Are in Russia, U....</td>\n",
       "      <td>http://twitter.com/danilina5886/statuses/98748...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>danilina5886 (danilina)</td>\n",
       "      <td>35339</td>\n",
       "      <td>30</td>\n",
       "      <td>nytimes.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42598</th>\n",
       "      <td>2001099695</td>\n",
       "      <td>MIPs test</td>\n",
       "      <td>2018-04-21 00:02:04</td>\n",
       "      <td>RT @GUDiplomacy Is #Austria's response to #Skr...</td>\n",
       "      <td>http://twitter.com/arenda/statuses/98748155692...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>neutral</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>...</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>USA.DC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arenda (Anthony Clark Arend)</td>\n",
       "      <td>249372</td>\n",
       "      <td>249260</td>\n",
       "      <td>lat.ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42599</th>\n",
       "      <td>2001099695</td>\n",
       "      <td>MIPs test</td>\n",
       "      <td>2018-04-21 00:01:41</td>\n",
       "      <td>Since the Skripal poisoning, their adopted hom...</td>\n",
       "      <td>http://twitter.com/NewsDingo/statuses/98748145...</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>negative</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NewsDingo (News Dingo)</td>\n",
       "      <td>4126</td>\n",
       "      <td>30</td>\n",
       "      <td>washingtonpost.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401847 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Query Id Query Name            datetime  \\\n",
       "3      2001100576  MainQuery 2018-03-06 23:59:40   \n",
       "5      2001100576  MainQuery 2018-03-06 23:59:28   \n",
       "7      2001100576  MainQuery 2018-03-06 23:59:01   \n",
       "9      2001100576  MainQuery 2018-03-06 23:58:36   \n",
       "12     2001100576  MainQuery 2018-03-06 23:58:22   \n",
       "...           ...        ...                 ...   \n",
       "42590  2001099695  MIPs test 2018-04-21 00:08:03   \n",
       "42596  2001099695  MIPs test 2018-04-21 00:03:22   \n",
       "42597  2001099695  MIPs test 2018-04-21 00:02:57   \n",
       "42598  2001099695  MIPs test 2018-04-21 00:02:04   \n",
       "42599  2001099695  MIPs test 2018-04-21 00:01:41   \n",
       "\n",
       "                                                   title  \\\n",
       "3      RT @W7VOA Sergei Skripal, who is 66, was grant...   \n",
       "5      RT @Billbrowder Very disturbing additional fac...   \n",
       "7      RT @Billbrowder Very disturbing additional fac...   \n",
       "9      RT @NBCNews \"The man went stiff. His arms stop...   \n",
       "12     RT @Billbrowder Very disturbing additional fac...   \n",
       "...                                                  ...   \n",
       "42590  Suspects in Attack on Ex-Spy Are in Russia, U....   \n",
       "42596  Independent Swiss Lab Says BZ Toxin Used in Sk...   \n",
       "42597  Suspects in Attack on Ex-Spy Are in Russia, U....   \n",
       "42598  RT @GUDiplomacy Is #Austria's response to #Skr...   \n",
       "42599  Since the Skripal poisoning, their adopted hom...   \n",
       "\n",
       "                                                     Url     platform  \\\n",
       "3      http://twitter.com/lindhays/statuses/971173499...  twitter.com   \n",
       "5      http://twitter.com/Tesscatbird/statuses/971173...  twitter.com   \n",
       "7      http://twitter.com/crlulukat/statuses/97117333...  twitter.com   \n",
       "9      http://twitter.com/tiffanyclay/statuses/971173...  twitter.com   \n",
       "12     http://twitter.com/KarCranky/statuses/97117317...  twitter.com   \n",
       "...                                                  ...          ...   \n",
       "42590  http://twitter.com/ElizabethFieshe/statuses/98...  twitter.com   \n",
       "42596  http://twitter.com/npnikk/statuses/98748188648...  twitter.com   \n",
       "42597  http://twitter.com/danilina5886/statuses/98748...  twitter.com   \n",
       "42598  http://twitter.com/arenda/statuses/98748155692...  twitter.com   \n",
       "42599  http://twitter.com/NewsDingo/statuses/98748145...  twitter.com   \n",
       "\n",
       "      Sentiment Page Type Language Country Code  ...                Region  \\\n",
       "3       neutral   twitter       en          USA  ...               Montana   \n",
       "5      negative   twitter       en          USA  ...            California   \n",
       "7      negative   twitter       en          NaN  ...                   NaN   \n",
       "9      negative   twitter       en          NaN  ...                   NaN   \n",
       "12     negative   twitter       en          USA  ...                   NaN   \n",
       "...         ...       ...      ...          ...  ...                   ...   \n",
       "42590   neutral   twitter       en          USA  ...                   NaN   \n",
       "42596   neutral   twitter       en          NaN  ...                   NaN   \n",
       "42597   neutral   twitter       en          NaN  ...                   NaN   \n",
       "42598   neutral   twitter       en          USA  ...  District of Columbia   \n",
       "42599  negative   twitter       en          NaN  ...                   NaN   \n",
       "\n",
       "      Region Code Root Blog Name Root Post Id Subreddit Subreddit Subscribers  \\\n",
       "3          USA.MT            NaN          NaN       NaN                   NaN   \n",
       "5          USA.CA            NaN          NaN       NaN                   NaN   \n",
       "7             NaN            NaN          NaN       NaN                   NaN   \n",
       "9             NaN            NaN          NaN       NaN                   NaN   \n",
       "12            NaN            NaN          NaN       NaN                   NaN   \n",
       "...           ...            ...          ...       ...                   ...   \n",
       "42590         NaN            NaN          NaN       NaN                   NaN   \n",
       "42596         NaN            NaN          NaN       NaN                   NaN   \n",
       "42597         NaN            NaN          NaN       NaN                   NaN   \n",
       "42598      USA.DC            NaN          NaN       NaN                   NaN   \n",
       "42599         NaN            NaN          NaN       NaN                   NaN   \n",
       "\n",
       "                                   Weblog Title user_id parent_user_id  \\\n",
       "3               lindhays (Democratic Socialism)       6              7   \n",
       "5                            Tesscatbird (Tess)      10             11   \n",
       "7                           crlulukat (Kathy 🌼)      12             11   \n",
       "9      tiffanyclay (❄️Evidence-Based Tiffany❄️)      14             15   \n",
       "12                           KarCranky (KarLen)      19             11   \n",
       "...                                         ...     ...            ...   \n",
       "42590    ElizabethFieshe (Pine Island Info Inc)   35405             30   \n",
       "42596                           npnikk (ツ Nick)   24406             30   \n",
       "42597                   danilina5886 (danilina)   35339             30   \n",
       "42598              arenda (Anthony Clark Arend)  249372         249260   \n",
       "42599                    NewsDingo (News Dingo)    4126             30   \n",
       "\n",
       "              news_domain  \n",
       "3               bbc.co.uk  \n",
       "5         theguardian.com  \n",
       "7         theguardian.com  \n",
       "9             nbcnews.com  \n",
       "12        theguardian.com  \n",
       "...                   ...  \n",
       "42590         nytimes.com  \n",
       "42596   independent.co.uk  \n",
       "42597         nytimes.com  \n",
       "42598              lat.ms  \n",
       "42599  washingtonpost.com  \n",
       "\n",
       "[401847 rows x 116 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['news_domain'].notnull()]\n",
    "df.to_csv(OUTPUT_DIR + \"\\\\all_osn_msgs.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4be29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = glob.glob(\"C:\\\\STUFF\\\\RESEARCH\\\\Brandwatch\\\\DATA\\\\MainQuery\\\\All\\\\\" + \"*.csv*\")[1]\n",
    "#file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d664a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(file_name, skiprows=6, parse_dates=['Date'],\n",
    "#                  dtype={'Twitter Author ID': str, 'Author':str,\n",
    "#                         'Full Text':str, 'Title':str,\n",
    "#                         'Thread Id':str, 'Thread Author':str,\n",
    "#                         'Domain':str, 'Expanded URLs':str})\n",
    "\n",
    "# df = df.rename(columns={'Date':'datetime', 'Author': 'source_user_id',\n",
    "#                         'Full Text':'content', 'Title':'title',\n",
    "#                         'Thread Id': 'parent_source_msg_id', 'Thread Author': 'parent_source_user_id',\n",
    "#                         'Domain':'platform', 'Expanded URLs':'article_url'})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a079e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
