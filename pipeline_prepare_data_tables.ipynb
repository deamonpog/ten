{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa844200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade networkx[default]\n",
    "#%pip install pyinform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c033a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqruied libraries\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pyinform\n",
    "\n",
    "# local functions import\n",
    "\n",
    "import transfer_entropy_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 specific libraries\n",
    "import s3fs\n",
    "import boto3\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30bcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running with S3 (inside AWS) ?\n",
    "RUNNING_IN_S3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "OUTPUT_DIR = './OUTPUT'\n",
    "\n",
    "#PC_INPUT_DIR = 'C:/STUFF/RESEARCH/Brandwatch/TEST'\n",
    "PC_INPUT_DIR = 'C:/STUFF/RESEARCH/Brandwatch/DATA/MainQuery/All'\n",
    "S3_INPUT_DIR = 's3://mips-main/initial_data_collection/raw_data/brandwatch'\n",
    "INPUT_DIR = S3_INPUT_DIR if RUNNING_IN_S3 else PC_INPUT_DIR\n",
    "\n",
    "S3_NEWS_DOMAINS_CSV_FILE = 's3://mips-main/initial_data_collection/news_outlets_v2.csv'\n",
    "PC_NEWS_DOMAINS_CSV_FILE = 'C:/STUFF/RESEARCH/Brandwatch/DATA/news_outlets_v2.csv'\n",
    "NEWS_DOMAINS_CSV_FILE = S3_NEWS_DOMAINS_CSV_FILE if RUNNING_IN_S3 else PC_NEWS_DOMAINS_CSV_FILE\n",
    "\n",
    "S3_NEWS_DOMAIN_TUFM_FILE = 's3://mips-main/initial_data_collection/processed_data/news_guard/news_table-v1-UT60-FM5.csv'\n",
    "PC_NEWS_DOMAIN_TUFM_FILE = 'C:/STUFF/RESEARCH/Brandwatch/DATA/news_table-v1-UT60-FM5.csv'\n",
    "NEWS_DOMAIN_TUFM_FILE = S3_NEWS_DOMAIN_TUFM_FILE if RUNNING_IN_S3 else PC_NEWS_DOMAIN_TUFM_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SAMPLE_FREQUENCY = 'D'\n",
    "skip_strings = {'add', 'al', 'au', 'ca', 'com', 'es', 'in', 'is', 'it', 'ms', 'my', 'net', 'news', 'org', 'rs', 'st', 'tv', 'uk', 'us', 'co'}\n",
    "MIN_COMM_SIZE = 500\n",
    "MIN_PLAT_SIZE = 50\n",
    "KNOWN_PLATFORMS = {'twitter.com', 'tumblr.com', 'youtube.com', 'reddit.com', '4chan.org', 'facebook.com', 'gab.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba34caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(data_directory, filter_platform_domain_set, is_using_s3):\n",
    "    \"\"\"\n",
    "    Reads data from all the csv files in the given directory\n",
    "    :param data_directory: Path to the directory that contains the csv files\n",
    "    :type data_directory: str\n",
    "    :return: pandas Dataframe that contains all the data from all csv files\n",
    "    :rtype: pd.Dataframe\n",
    "    \"\"\"\n",
    "    data_files = []\n",
    "    if is_using_s3:\n",
    "        data_files = s3.glob(os.path.join(data_directory, \"*.csv*\"))\n",
    "    else:\n",
    "        data_files = glob.glob(os.path.join(data_directory, \"*.csv*\"))\n",
    "    prefix_path = ''\n",
    "    if is_using_s3:\n",
    "        prefix_path = 's3://'\n",
    "    print(data_files)\n",
    "    df_list = []\n",
    "    for idx, file in enumerate(data_files):\n",
    "        print(f\"Reading {idx + 1} of {len(data_files)} files.\\nFile name: {file}\")\n",
    "        df = pd.read_csv(prefix_path + data_files[idx], skiprows=6, parse_dates=['Date'],\n",
    "                         dtype={'Twitter Author ID': str, 'Author':str,\n",
    "                                'Full Text':str, 'Title':str,\n",
    "                                'Thread Id':str, 'Thread Author':str,\n",
    "                                'Domain':str, 'Expanded URLs':str,\n",
    "                                'Avatar':str, 'Parent Blog Name':str, 'Root Blog Name':str})\n",
    "        # df = df[['Date', 'Hashtags', 'Twitter Author ID', 'Author', 'Url', 'Thread Id', 'Thread Author', 'Domain']]\n",
    "        df = df.rename(columns={'Date':'datetime', 'Author': 'source_user_id',\n",
    "                                'Full Text':'content', 'Title':'title',\n",
    "                                'Thread Id': 'parent_source_msg_id', 'Thread Author': 'parent_source_user_id',\n",
    "                                'Domain':'platform', 'Expanded URLs':'article_url'})\n",
    "        df_list.append(df)\n",
    "        \n",
    "    start_time = time.time() \n",
    "    result_df = pd.concat(df_list)\n",
    "    end_time = time.time() \n",
    "    print(f\"{(end_time - start_time)/60} mins for concat dataframes\")\n",
    "    \n",
    "    start_time = time.time() \n",
    "    result_df.drop_duplicates(subset='Url', keep=\"first\", inplace=True)\n",
    "    end_time = time.time() \n",
    "    print(f\"{(end_time - start_time)/60} mins for drop duplicates\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result_df['platform'].value_counts().rename('users_count').rename_axis('platform').to_csv(OUTPUT_DIR + \"/platform_counts_info.csv\")\n",
    "    result_df = result_df[result_df['platform'].isin(filter_platform_domain_set)]\n",
    "    end_time = time.time() \n",
    "    print(f\"{(end_time - start_time)/60} mins for filtering platforms\")\n",
    "    \n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "    print(result_df.shape)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa215f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# read data and filter for only the KNOWN_PLATFORMS\n",
    "all_osn_msgs_df = read_csv_data(INPUT_DIR, KNOWN_PLATFORMS, RUNNING_IN_S3)\n",
    "all_osn_msgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_osn_msgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add user_id column and parent_user_id column\n",
    "\n",
    "def generate_users_dict(osn_msgs_df):\n",
    "    global next_user_id\n",
    "    users_data = {}\n",
    "    next_user_id = 0\n",
    "    def extract_user(row):\n",
    "        global next_user_id\n",
    "        if (row['platform'], row['source_user_id']) not in users_data:\n",
    "            users_data[(row['platform'], row['source_user_id'])] = next_user_id\n",
    "            next_user_id += 1\n",
    "        if (row['platform'], row['parent_source_user_id']) not in users_data:\n",
    "            users_data[(row['platform'], row['parent_source_user_id'])] = next_user_id\n",
    "            next_user_id += 1\n",
    "    all_osn_msgs_df.apply(lambda row: extract_user(row), axis=1)\n",
    "    print(len(users_data))\n",
    "    return users_data\n",
    "\n",
    "users_data = generate_users_dict(all_osn_msgs_df)\n",
    "\n",
    "users_df = pd.Series(users_data).rename_axis(['platform','source_user_id']).rename('user_id').reset_index()\n",
    "print(users_df.shape)\n",
    "print(users_df)\n",
    "users_df.to_csv(OUTPUT_DIR + \"/users.csv\",index=False)\n",
    "\n",
    "# add user_id column and parent_user_id column\n",
    "all_osn_msgs_df[['user_id','parent_user_id']] = all_osn_msgs_df.apply(lambda row: pd.Series([\n",
    "            users_data[(row['platform'],row['source_user_id'])],\n",
    "            users_data[(row['platform'],row['parent_source_user_id'])]\n",
    "        ]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add news_domains column\n",
    "\n",
    "def find_patterns_for_domains(in_news_domains_csv_file, in_skip_strings):\n",
    "    news_domains = pd.read_csv(in_news_domains_csv_file)['news outlets'].rename('news_outlets')\n",
    "    news_domains_set = set(news_domains.to_list())\n",
    "    pattern_to_news_domain_name = {re.compile(nd):nd for nd in news_domains_set}\n",
    "    for nd in news_domains_set:\n",
    "        valid_split_strs = set(nd.split('.'))\n",
    "        for e in in_skip_strings:\n",
    "            valid_split_strs.discard(e)\n",
    "        for sp in valid_split_strs:\n",
    "            if len(sp) > 2:\n",
    "                pattern_to_news_domain_name[re.compile(sp)] = nd\n",
    "    return pattern_to_news_domain_name\n",
    "\n",
    "def search_domain_in_string(in_expanded_url, in_news_domains_names):\n",
    "    # print(in_expanded_url)\n",
    "    max_len_match = None\n",
    "    max_len_found = 0\n",
    "    for ndn in in_news_domains_names:\n",
    "        match_obj = ndn.search(in_expanded_url)\n",
    "        if match_obj:\n",
    "            # print(match_obj, in_news_domains_names[ndn], match_obj.end() - match_obj.start())\n",
    "            if max_len_found < match_obj.end() - match_obj.start():\n",
    "                max_len_match = ndn\n",
    "                max_len_found = match_obj.end() - match_obj.start()\n",
    "    return in_news_domains_names[max_len_match] if max_len_match is not None else None\n",
    "\n",
    "\n",
    "def calculate_news_domain_series(in_string_series, in_skip_strings):\n",
    "    news_domains_names = find_patterns_for_domains(NEWS_DOMAINS_CSV_FILE, in_skip_strings)\n",
    "    return in_string_series.apply(lambda x: search_domain_in_string(x, news_domains_names) if type(x) is str else None)\n",
    "\n",
    "all_osn_msgs_df['news_domain'] = calculate_news_domain_series(all_osn_msgs_df['article_url'], skip_strings)\n",
    "all_osn_msgs_df = all_osn_msgs_df.loc[all_osn_msgs_df['news_domain'].notnull(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_osn_msgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read TUMF class values of news_domains\n",
    "tufmdf = pd.read_csv(NEWS_DOMAIN_TUFM_FILE)\n",
    "tufmdf.rename(columns={'Domain':'news_domain'}, inplace=True)\n",
    "print(\"raw: \" ,tufmdf['tufm_class'].unique())\n",
    "def get_clearn_tufm_class(row):\n",
    "    if row['tufm_class'] == '0':\n",
    "        return 'UF'\n",
    "    else:\n",
    "        return row['tufm_class']\n",
    "    \n",
    "tufmdf['clean_tufm_class'] = tufmdf.apply(lambda row: get_clearn_tufm_class(row), axis=1)\n",
    "tufmdf = tufmdf[['news_domain','clean_tufm_class']].rename(columns={'clean_tufm_class':'class'})\n",
    "print(\"cleaned:\", tufmdf['class'].unique())\n",
    "print(tufmdf)\n",
    "news_domain_to_tufm_class = tufmdf.set_index('news_domain')['class'].to_dict()\n",
    "del tufmdf\n",
    "news_domain_to_tufm_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TUFMClass to osn messages table\n",
    "all_osn_msgs_df['class'] = all_osn_msgs_df['news_domain'].apply(lambda x: news_domain_to_tufm_class[x])\n",
    "print(all_osn_msgs_df['class'].unique())\n",
    "all_osn_msgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6814754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# generate retweet network\n",
    "retweet_network_edges = all_osn_msgs_df.groupby(['parent_user_id','user_id'], as_index=False).size().sort_values('size', ascending=False).rename(columns={'size':'num_retweets'})\n",
    "print(retweet_network_edges)\n",
    "retweet_network_edges.to_csv(OUTPUT_DIR + \"/retweet_network.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# generate community partition on retweet network\n",
    "G = nx.from_pandas_edgelist(retweet_network_edges,'parent_user_id','user_id',['num_retweets'])\n",
    "print(G)\n",
    "lc = nx.algorithms.community.louvain_communities(G, resolution=1, seed=235246345345)\n",
    "print(len(lc))\n",
    "user_comm_sizes = pd.Series([len(c) for c in lc]).value_counts().rename_axis('community_size').rename('num_of_communities').reset_index().sort_values('community_size', ascending=False).reset_index(drop=True).rename_axis('size_rank')\n",
    "user_comm_sizes.to_csv(OUTPUT_DIR + \"/all_user_comm_sizes.csv\")\n",
    "print(user_comm_sizes[user_comm_sizes['community_size'] > 500])\n",
    "filtered_comms = [c for c in lc if len(c) > MIN_COMM_SIZE]\n",
    "print(f\"Number of communities with min of {MIN_COMM_SIZE} users : {len(filtered_comms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# generate actor tables \n",
    "\n",
    "# generate platform actors table\n",
    "platform_to_user_count = all_osn_msgs_df['platform'].value_counts().to_dict()\n",
    "platform_actors_df = pd.Series(platform_to_user_count).rename('users_count').rename_axis('platform').reset_index()\n",
    "platform_actors_df = platform_actors_df[(platform_actors_df['users_count'] > MIN_PLAT_SIZE) & (platform_actors_df['platform'].isin(KNOWN_PLATFORMS))].reset_index(drop=True).rename_axis('actor_id')\n",
    "next_actor_id = platform_actors_df.index.max() + 1\n",
    "print(next_actor_id)\n",
    "print(\"plat_actors:\\n\", platform_actors_df, \"\\n\")\n",
    "platform_actors_df.to_csv(OUTPUT_DIR + \"/plat_actors.csv\")\n",
    "\n",
    "# generate individual actors table\n",
    "ind_actors_df = all_osn_msgs_df['parent_user_id'].value_counts().iloc[:100].rename('received_share_count').rename_axis('user_id').reset_index().rename_axis('actor_id')\n",
    "ind_actors_df.index = ind_actors_df.index + next_actor_id\n",
    "ind_actors_df.reset_index(inplace=True)\n",
    "next_actor_id = ind_actors_df['actor_id'].max() + 1\n",
    "print(next_actor_id)\n",
    "print(\"indv_actors:\\n\", ind_actors_df, \"\\n\")\n",
    "ind_actors_df.to_csv(OUTPUT_DIR + \"/indv_actors.csv\", index=False)\n",
    "\n",
    "# generate community actors table\n",
    "comm_info_df = pd.DataFrame([[len(c), c] for c in filtered_comms], columns=['users_count', 'users_set']).sort_values('users_count', ascending=False).reset_index(drop=True).rename_axis('comm_id')\n",
    "comm_info_df['actor_id'] = comm_info_df.index + next_actor_id\n",
    "comm_info_df = comm_info_df.reset_index().set_index('actor_id')\n",
    "comm_info_df.to_csv(OUTPUT_DIR + \"/comm_info.csv\")\n",
    "print(\"Community info :\\n\", comm_info_df)\n",
    "comm_actors_data = []\n",
    "for actid,row in comm_info_df.iterrows():\n",
    "    for uid in row['users_set']:\n",
    "        comm_actors_data.append([uid, row['comm_id'], actid])\n",
    "        \n",
    "comm_actors_df = pd.DataFrame(comm_actors_data, columns=['user_id','comm_id','actor_id'])\n",
    "print(\"comm_actors:\\n\", comm_actors_df, \"\\n\")\n",
    "comm_actors_df.to_csv(OUTPUT_DIR + \"/comm_actors.csv\", index=False)\n",
    "\n",
    "\n",
    "# generate actors table\n",
    "all_actors = [[plt_aid, platform_actors_df.loc[plt_aid]['platform'], 'plat', platform_actors_df.loc[plt_aid]['users_count']] for plt_aid in platform_actors_df.index.to_list()]\n",
    "all_actors = all_actors + [[indv,\n",
    "                            users_df[ users_df['user_id'] == ind_actors_df[ind_actors_df['actor_id']==indv].iloc[0]['user_id'] ].iloc[0]['source_user_id'],\n",
    "                            'indv', 1] for indv in ind_actors_df['actor_id']]\n",
    "print('here')\n",
    "all_actors = all_actors + list({(comm_aid, comm_info_df.loc[comm_aid]['comm_id'], 'comm', comm_info_df.loc[comm_aid]['users_count']) for comm_aid in comm_info_df.index})\n",
    "print('here')\n",
    "all_actors_df = pd.DataFrame(all_actors, columns=['actor_id','actor_label','actor_type','num_users']).sort_values('actor_id')\n",
    "all_actors_df.to_csv(OUTPUT_DIR + \"/actors.csv\", index=False)\n",
    "all_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_osn_msgs_df.to_csv(OUTPUT_DIR + \"/all_osn_msgs.csv\", index=False)\n",
    "all_osn_msgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload datafiles to s3 bucket\n",
    "bucket_name = 'mips-main'\n",
    "folder_name = 'initial_data_collection/processed_data/actors_and_messages/v1'\n",
    "b = boto3.Session().resource('s3').Bucket(bucket_name)\n",
    "for file_name in [os.path.basename(f) for f in glob.glob(\"./OUTPUT/*.csv\")]:\n",
    "    print(f\"Uploading file : {OUTPUT_DIR}/{file_name} to {bucket_name}/{folder_name}/{file_name}\")\n",
    "    b.upload_file(f\"{OUTPUT_DIR}/{file_name}\", f\"{folder_name}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ac732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(transfer_entropy_functions)\n",
    "\n",
    "def compute_actor_te_network(actors_of_interest, t3_all_osn_msgs, t6_actors, t7_indv_actors, t8_comm_actors, t9_plat_actors, freq):\n",
    "    # generate te network for the selected actors\n",
    "    print(t6_actors)\n",
    "    results_df = transfer_entropy_functions.generate_te_edge_list(actors_of_interest, t3_all_osn_msgs, t6_actors, t7_indv_actors, t8_comm_actors, t9_plat_actors, freq)\n",
    "    results_df.to_csv(OUTPUT_DIR + '/actor_te_edges.csv', index=False)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fcbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52e952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad7e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_actors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544dbe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_osn_msgs_df.iloc[541].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actors_df = all_actors_df.set_index('actor_id')\n",
    "ind_actors_df = ind_actors_df.set_index('actor_id')\n",
    "comm_actors_df = comm_actors_df.set_index('actor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e8aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(transfer_entropy_functions)\n",
    "r = compute_actor_te_network(all_actors_df.index.to_list(), all_osn_msgs_df, all_actors_df, ind_actors_df, comm_actors_df, platform_actors_df,\n",
    "                             SAMPLE_FREQUENCY)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"num of TE <= 0 edges : {r[r['total_te'] <= 0].shape[0]}\")\n",
    "print(f\"num of TE > 0 edges : {r[r['total_te'] > 0].shape[0]}\")\n",
    "r[r['total_te'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.upload_file(f\"{OUTPUT_DIR}/actor_te_edges.csv\", f\"{folder_name}/actor_te_edges.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actors_df.actor_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2ddb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc06fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
